---
title: Random Forest for Trees
author: Willian Vieira
date: "`r paste('Last updated on', format(Sys.time(), '%d %B, %Y'))`"

---

```{r load R packages, echo=FALSE}
suppressPackageStartupMessages(library(ggplot2))
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(DT))
```

```{r getting variables, echo=FALSE}

  # species_id
  sp_ids = as.character(read.table('data/sp_ids.txt')[, 1])

  # other variables from the simulation file
  rFile <- readLines('R/03_runSimulations.R')

  # variables
  eval(parse(text =
    rFile[which(sapply(rFile, function(x) grep('variables = ', x)) == 1)]))

  # nbTrees
  eval(parse(text =
    rFile[which(sapply(rFile, function(x) grep('nbTrees = ', x)) == 1)]))

  # nbMtry
  eval(parse(text =
    rFile[which(sapply(rFile, function(x) grep('nbMtry = ', x)) == 1)]))

```

# Introduction

Here I use the random forest method to extract the power of different variables to explain **growth** and **mortality** of tree species.
I use the forest inventory data base for the eastern North American forest.

```{r explanatory variables, echo=FALSE}
  vars = as.character(read.table('data/variables1.txt')[, 1])
```

For each tree species in the data base (total of `r length(sp_ids)`), we will run a random forest for growth and mortality with different parameters and explanatory variables.
Then we evaluate the simulation based on the R$^2$ for the growth and the out-of-bag estimate (1 - OOB) for mortality.
For mortality, in which alive events are much more frequent than dead events, we avoid overestimating OOB by balancing the sampling weights to have the same number of observations from each class as suggested in Janitza & Hornung ([2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0201904)).
We further check which variables have a better explanatory power over the response variable.

# Simulations

For each tree species, we ran a random forest varying (i) the explanatory variables, and (ii) the number of variables to possibly split at in each node (`mtry`).
The `r length(variables)` sets of variables are the following:

```{r table of variables, echo=FALSE}
# get variables
for(var in variables) assign(paste0('var', var), read.table(paste0('data/variables', var, '.txt'))[, 1])
for(var in variables) {print(paste('set', var)); print(as.character(get(paste0('var', var)))); cat('\n')}
```

For each set of variables, `mtry` varied as `r nbMtry`, with a fixed number of `r nbTrees` trees.

# Summary

```{r prepare data, echo=FALSE}
# data frame to save explanatory power
dfPw <- expand.grid(c('growth', 'mort'), sp_ids, variables, nbTrees, nbMtry)
names(dfPw) <- c('vital', 'sp', 'var', 'nbTrees', 'nbMtry')
dfPw$var <- as.factor(dfPw$var)
dfPw$nbTrees <- as.factor(dfPw$nbTrees)
dfPw$nbMtry <- as.factor(dfPw$nbMtry)
dfPw$pw <- rep(NA, nrow(dfPw))

# add species information
sp_info <- read.table('data/sp_info.txt')
dfPw <- merge(dfPw, sp_info[, c(1, 2, 5)], by.x = 'sp', by.y = 'species_id')

# reorganize df
dfPw <- dfPw[, c(2, 1, 7, 8, 3, 4, 5, 6)]

# data frame to get importance
dfImportance <- as.data.frame(matrix(rep(NA, length(vars) * nrow(dfPw)), ncol = length(vars)))
names(dfImportance) <- vars

# load each simulation and import all information
for(vital in c('growth', 'mort')) {
  for(sp in sp_ids) {
    for(var in variables) {
      for(tree in nbTrees) {
        for(mty in nbMtry) {

          # get dfPw row
          Row <- which(dfPw$vital == vital & dfPw$sp == sp & dfPw$var == var &dfPw$nbTrees == tree & dfPw$nbMtry == mty)

          # load simulation
          sim <- readRDS(paste0('output/', vital, '_', sp, '_var', var, '_nTrees', nbTrees, '_Mtry', mty, '.RDS'))

          # get explanatory power
          if(sim$treetype == 'Regression') {
            dfPw[Row, 'pw'] = sim$r.squared
          }else if(sim$treetype == 'Classification') {
            dfPw[Row, 'pw'] = 1 - sim$prediction.error
          }

          # get importance
          impRF <- ranger::importance(sim)
          dfImportance[Row, names(impRF)] <- impRF

        }
      }
    }
  }
}

# tranform to long format
dfLong = gather(cbind(dfPw, dfImportance), variable, importance, growth:tot_pp_period3_lag, factor_key = TRUE)


## Get sample size with median pw
pwSp <- dfPw %>%
        group_by(vital, sp) %>%
        summarise(median = median(pw))

# add training size
pwSp <- merge(pwSp, read.table('data/trainingSize_spIds.txt'), by.x = c('vital', 'sp'), by.y = c('vital', 'sp'))

# add species info
pwSp <- merge(pwSp, sp_info[, 1:2], by.x = 'sp', by.y = 'species_id')
```

## Variables set

Which variable set explains the best for growth and mortality?

```{r variables set, echo=FALSE}
p = ggplot(data = dfPw) +
    aes(var, pw) +
    geom_boxplot() +
    facet_grid(~vital,
              labeller = labeller(vital = setNames(c('growth (Rsquared)',
                                  'mort (1 - OOB prediction error)'),
                                  c('growth', 'mort')))) +
    theme_classic() +
    xlab("Variable set") +
    ylab("Explanatory power of variables")

print(p)
```

Assuming the set of variables `var2` was the best to explain growth and mortality, let's see the importance of the variables present in the set `var2`:

```{r variable set importance growth, echo=FALSE}
p = ggplot(data = droplevels(subset(dfLong, var == 2 & variable %in% var2))) +
    aes(reorder(variable, importance, FUN = median), log(importance)) +
    geom_boxplot() +
    coord_flip() +
    facet_grid(~vital, scales = "free_x") +
    theme_classic() +
    xlab("Variables in set 2") +
    ylab("Importance (log)")

suppressWarnings(print(p))
```

Now for the other two sets of variables (`var1` and `var3`):

```{r variable set importance2, echo=FALSE}
p = ggplot(data = droplevels(subset(dfLong, var == 1 & variable %in% var1))) +
    aes(reorder(variable, importance, FUN = median), log(importance)) +
    geom_boxplot() +
    coord_flip() +
    facet_grid(~vital, scales = "free_x") +
    theme_classic() +
    xlab("Variables in set 1") +
    ylab("Importance (log)")

suppressWarnings(print(p))
```

```{r variable set importance3, echo=FALSE}
p = ggplot(data = droplevels(subset(dfLong, var == 3 & variable %in% var3))) +
    aes(reorder(variable, importance, FUN = median), log(importance)) +
    geom_boxplot() +
    coord_flip() +
    facet_grid(~vital, scales = "free_x") +
    theme_classic() +
    xlab("Variables in set 3") +
    ylab("Importance (log)")

suppressWarnings(print(p))
```

## Number of variables to possibly split at in each node

```{r mtry, echo=FALSE}
p = ggplot(data = dfPw) +
    aes(nbMtry, pw) +
    geom_boxplot() +
    facet_grid(~vital,
              labeller = labeller(vital = setNames(c('growth (Rsquared)',
                                  'mort (1 - OOB prediction error)'),
                                  c('growth', 'mort')))) +
    theme_classic() +
    xlab("Number of variables to possibly split at in each node (mtry)") +
    ylab("Explanatory power of variables")

print(p)
```

## Individual species response

Which species perform the better?

```{r species_id, echo=FALSE}
p = ggplot(data = dfPw) +
    aes(reorder(latin, pw, FUN = median), pw) +
    geom_boxplot() +
    coord_flip() +
    facet_grid(~vital,
              labeller = labeller(vital = setNames(c('growth (Rsquared)',
                                  'mort (1 - OOB prediction error)'),
                                  c('growth', 'mort')))) +
    theme_classic() +
    xlab("Species") +
    ylab("Explanatory power of variables")

print(p)
```

What was the best predictor for each species?

```{r best predictors by species, echo=FALSE}

## Get sample size with median pw
bestPredicSp <- dfLong %>%
                group_by(vital, latin) %>%
                summarise(bestPredic = variable[which.max(importance)])

bestPredicWide <- spread(bestPredicSp, vital, bestPredic)

DT::datatable(bestPredicWide)
```

And when species are grouped by tolerance to shade? _**H**igh, **M**edium, and **L**ow tolerance to shade_.

```{r shade tolerance, echo=FALSE}
p = ggplot(data = droplevels(subset(dfLong, var == 2 & variable %in% var2))) +
    aes(reorder(tolLevel, importance, FUN = median), importance) +
    geom_boxplot() +
    coord_flip() +
    facet_grid(variable~vital, scales = "free_x") +
    theme_classic() +
    xlab("Tolerance level") +
    ylab("Importance")

suppressWarnings(print(p))
```

```{r shade tolerance table, echo=FALSE}
bestPredicSp <- dfLong %>%
                group_by(vital, tolLevel) %>%
                summarise(bestPredic = variable[which.max(importance)])

bestPredicWide <- spread(bestPredicSp, vital, bestPredic)

DT::datatable(bestPredicWide)
```


Is there a correlation between explanatory power and sample size?

```{r species_id vs n, echo=FALSE}
p = ggplot(data = pwSp) +
    aes(trainingSize, median) +
    geom_point() +
    geom_smooth(method='lm', formula= y ~ x + I(x^2)) +
    facet_grid(~vital,
              labeller = labeller(vital = setNames(c('growth (Rsquared)',
                                  'mort (1 - OOB prediction error)'),
                                  c('growth', 'mort')))) +
    theme_classic() +
    xlab("Sample size") +
    ylab("Explanatory power of variables")

print(p)
```

```{r table with performance and n, echo=FALSE}
names(pwSp)[3] <- 'Explain'
pwSp$Explain <- round(pwSp$Explain, 3)

perform <- pivot_wider(data = pwSp,
                 id_cols = latin,
                 names_from = vital,
                 values_from = c("Explain", "trainingSize"))

DT::datatable(perform)
```
